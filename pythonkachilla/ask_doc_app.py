import streamlit as st
from transformers import pipeline
from langchain.text_splitter import CharacterTextSplitter
import io

import streamlit as st
from transformers import pipeline
from langchain.text_splitter import CharacterTextSplitter
import io
import re

# Choose a suitable Hugging Face model for question answering
model_name = "deepset/roberta-base-squad2"  # Example model, replace with your choice

def process_document(uploaded_file):
    """
    Reads text content from uploaded PDF or DOCX using appropriate libraries.

    Args:
        uploaded_file (streamlit.uploadedfile.UploadedFile): The uploaded file.

    Returns:
        str: The extracted text content from the file.
    """

    if uploaded_file.type in ["application/pdf", "application/vnd.openxmlformats-officedocument.wordprocessingml.document"]:
        # Use appropriate library (PyPDF2, docx) based on file type
        if uploaded_file.type == "application/pdf":
            import PyPDF2

            # Option 1 (recommended): Use io.BytesIO
            file_object = io.BytesIO(uploaded_file.read())
            reader = PyPDF2.PdfReader(file_object)

            text = ""
            for page in reader.pages:
                text += page.extract_text()
        else:
            # Handle DOCX using a suitable library like docx2txt
            import docx2txt
            text = docx2txt.process(uploaded_file.read())

        # Split text into paragraphs
        paragraphs = re.split(r'\n\s*\n', text)
        return paragraphs

    else:
        st.error("Unsupported file type. Please upload a PDF or DOCX document.")
        return None

def answer_question(question, paragraphs):
    """
    Sends the question and processed text to the Hugging Face model for answer generation.

    Args:
        question (str): The user's question about the document.
        paragraphs (list): List of paragraphs extracted from the document.

    Returns:
        str: The answer generated by the Hugging Face model.
    """

    # Concatenate paragraphs into a single text
    context = "\n".join(paragraphs)

    qa_pipeline = pipeline("question-answering", model=model_name)  # Create pipeline with chosen model
    answer = qa_pipeline(question=question, context=context)["answer"]
    return answer

st.set_page_config(page_title="Document Reader & Questioner", layout="wide")

st.title("Read and Ask Questions about Your Documents")
st.write("Upload a PDF or DOCX document and ask your questions. This app uses Langchain and a Hugging Face model to help you find information within.")

uploaded_file = st.file_uploader("Choose a document:", type=["pdf", "docx"])

if uploaded_file is not None:
    paragraphs = process_document(uploaded_file)
    if paragraphs:
        question = st.text_input("Ask a question about the document:")

        if question:
            answer = answer_question(question, paragraphs)
            st.success("**Answer:**")
            st.write(answer)

